"Thème","Question","Réponse Structurée","Points Clés"
"Fondamentaux ML","Expliquez la différence entre biais et variance. Comment les réduire ?","Biais : Erreur due à une modélisation trop simple (underfitting). Variance : Sensibilité aux variations des données (overfitting). Réduction : Plus de données, régularisation (L1/L2), cross-validation, feature engineering, ensembles (bagging/boosting).","Trade-off biais-variance, généralisation, complexité du modèle."
"","Quand utiliser un Random Forest plutôt qu’un Gradient Boosting ?","Random Forest : Robuste, parallèle, peu de tuning, bon pour données bruitées. Gradient Boosting : Meilleure précision (si bien tuné), mais sensible au bruit et plus lent. Utiliser RF pour rapidité/interprétabilité, GB pour performance maximale.","Interprétabilité, temps d’entraînement, sensibilité au bruit."
"","Comment choisissez-vous entre un modèle paramétrique et non paramétrique ?","Paramétrique (régression linéaire) : Données structurées, peu de features, besoin d’interprétabilité. Non paramétrique (k-NN, SVM, arbres) : Données complexes, non linéaires, beaucoup de features.","Flexibilité, risque d’overfitting, scalabilité."
"","Qu’est-ce que le surapprentissage ? Comment le prévenir ?","Surapprentissage : Modèle trop complexe, performant en training mais pas en test. Prévention : Régularisation, early stopping, pruning (arbres), plus de données, cross-validation.","Généralisation, complexité, validation."
"","Comment gérez-vous les données manquantes ou déséquilibrées ?","Manquantes : Imputation (moyenne/médiane), suppression, modèles (KNN, MICE). Déséquilibrées : Resampling (SMOTE, ADASYN), class weighting, métriques adaptées (F1, AUC-ROC), focal loss.","Qualité des données, biais algorithmique, métriques."
"Conception Systèmes ML","Comment concevriez-vous un système de recommandation pour Netflix ?","1. Collecte : Logs utilisateurs (clics, temps de visionnage). 2. Stockage : Data Lake (S3) + Feature Store (Feast). 3. Modélisation : Collaborative filtering (ALS) + deep learning (embeddings). 4. Serving : API (FastAPI) + caching (Redis). 5. Monitoring : A/B testing, drift detection (Evidently).","Scalabilité, personnalisation, latence, feedback loop."
"","Quels sont les défis du déploiement d’un modèle en production ?","Déploiement : Versionnage, rollback, A/B testing. Monitoring : Performance, drift, latence. Maintenance : Retraining, feedback loop, scalabilité.","Fiabilité, observabilité, automatisation, CI/CD."
"Traitement Données","Comment optimisez-vous un pipeline Spark pour réduire les coûts ?","Partitionnement : Taille optimale (128Mo-1Go). Caching : Réutilisation des DataFrames. Broadcast Join : Pour les petites tables. Tuning : spark.executor.memory, parallelism, Dynamic Resource Allocation.","Performance, coût, mémoire, parallélisme."
"","Quelles techniques utilisez-vous pour la sélection de features ?","Filtres : Chi2, mutual info. Wrapper : RFE, forward/backward selection. Embedded : Lasso, feature importance (arbres). Outils : Scikit-learn, MLlib.","Réduction de dimension, interprétabilité, performance."
"Optimisation Modèles","Quelles métriques utilisez-vous pour évaluer un modèle de classification binaire ?","Précision/Recall : Équilibre faux positifs/négatifs. AUC-ROC : Robuste aux classes déséquilibrées. F1 : Harmonic mean précision/recall. Log Loss : Confiance des prédictions. Business metrics : ROI, coût par erreur.","Adaptation au cas d’usage, trade-offs."
"","Comment optimisez-vous les hyperparamètres d’un modèle ?","Méthodes : Grid search, random search, Bayesian optimization. Outils : Optuna, Hyperopt, MLflow. Bonnes pratiques : Cross-validation, early stopping, logging.","Efficacité, reproductibilité, automatisation."
"Deep Learning","Expliquez le mécanisme d’attention dans les Transformers.","Attention : Mécanisme de pondération (QKV) pour capturer les dépendances entre tokens. Multi-Head Attention : Plusieurs heads pour capturer différents patterns. Avantages : Parallélisable, capture les dépendances longues, SOTA en NLP/vision.","Interprétabilité, scalabilité, performance."
"","Comment entrainez-vous un modèle avec peu de données ?","Data Augmentation : Rotation, bruit, SMOTE. Transfer Learning : Fine-tuning de modèles pré-entraînés (BERT, ResNet). Regularisation : Dropout, weight decay. Semi-supervised : Self-training, consistency regularization.","Généralisation, coût, performance."
"Déploiement/MLOps","Quels outils utilisez-vous pour déployer des modèles en production ?","Serving : TFServing, TorchServe, KServe. Orchestration : Kubernetes, Docker. Monitoring : Prometheus, Evidently. CI/CD : GitHub Actions, MLflow, Kubeflow.","Scalabilité, reproductibilité, maintenance."
"","Comment surveillez-vous un modèle en production ?","Métriques : Performance (AUC, RMSE), latence, throughput. Dérive : Data drift (statistiques), concept drift (performance). Outils : Evidently, Arize, Prometheus. Alertes : Slack, PagerDuty.","Observabilité, maintenance proactive, feedback loop."
"Architecture Data/IA","Comment concevriez-vous une architecture Data/IA pour une plateforme temps réel ?","1. Ingestion : Kafka (streaming) + Airflow (batch). 2. Stockage : Delta Lake (lakehouse) + Snowflake. 3. Traitement : Spark/Flink. 4. Serving : Feature Store (Feast) + API (FastAPI). 5. Monitoring : Grafana + Evidently.","Latence, cohérence, scalabilité, résilience."
"","Quelle est la différence entre Data Lake, Data Warehouse et Lakehouse ?","Data Lake : Données brutes (S3, non structurées). Data Warehouse : SQL, analytique (Snowflake, structuré). Lakehouse : ACID + ML (Delta Lake, Iceberg).","Flexibilité, coût, performance, gouvernance."
"Pipelines ETL/ELT","Comment gérez-vous les données en temps réel ?","Outils : Kafka (ingestion), Flink (processing), Redis (caching). Défis : Latence, cohérence, tolérance aux pannes. Solutions : Exactly-once processing, stateful operations, monitoring.","Streaming, scalabilité, fiabilité."
"","Comment assurez-vous la qualité des données dans un pipeline ?","Tests : Great Expectations (validations). Monitoring : Métriques (complétude, unicité), alertes. Gouvernance : Data lineage (OpenLineage), catalogues (Amundsen).","Traçabilité, reproductibilité, confiance."
"Serving/Inférence","Comment déployez-vous un modèle avec une latence <100ms ?","Optimisation : Quantization (TF-Lite), ONNX, TensorRT. Serving : TFServing + Redis (caching). Infrastructure : GPU, serverless (Lambda), microservices.","Latence, throughput, coût, hardware."
"","Comment gérez-vous le versionnage et le rollback de modèles ?","Outils : MLflow (versionnage), feature flags, canary deployment. Stratégies : A/B testing, shadow deployment, monitoring.","Sécurité, flexibilité, audit."
"Sécurité/Gouvernance","Comment sécurisez-vous les données sensibles ?","Chiffrement : KMS (au repos), TLS (en transit). Accès : IAM, RBAC. Anonymisation : Masking, tokenization. Conformité : RGPD (droit à l’oubli), logs d’audit.","Confidentialité, conformité, traçabilité."
"Cloud/Infrastructure","Quels sont les avantages/inconvénients d’AWS vs GCP vs Azure ?","AWS : Écosystème large, coûts complexes. GCP : BigQuery/Vertex AI intégrés, pricing simple. Azure : Intégration Microsoft, hybride. Critères : Services managed, lock-in, coûts.","Flexibilité, coût, écosystème."
"","Comment utilisez-vous l’Infrastructure as Code (IaC) ?","Outils : Terraform, Pulumi, CloudFormation. Avantages : Reproductibilité, versionnage, collaboration. Cas d’usage : Déploiement multi-cloud, scalabilité.","DevOps, automatisation, maintenabilité."
"Études de Cas","Comment diagnostiquez-vous une chute de performance en production ?","1. Logs : Erreurs, latence. 2. Métriques : Performance (AUC), dérive. 3. Profiling : Bottlenecks (CPU/mémoire). 4. Tests : A/B, rollback. Outils : Evidently, Prometheus.","Méthodologie, outils, réactivité."
"","Comment concevriez-vous un système de détection de fraude en temps réel ?","1. Ingestion : Kafka. 2. Traitement : Flink (streaming), feature store. 3. Modèle : Random Forest/GBM (interprétabilité), ou deep learning (performance). 4. Serving : TFServing + Redis. 5. Monitoring : Alertes (Slack), feedback loop.","Latence, précision, scalabilité."
"Questions Comportementales","Comment gérez-vous un désaccord technique dans une équipe ?","Écoute : Comprendre les arguments. Données : Tests, benchmarks. Décision : Consensus ou arbitrage (lead). Suivi : Rétrospective post-décision.","Collaboration, leadership, communication."